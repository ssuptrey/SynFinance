{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d48057",
   "metadata": {},
   "source": [
    "# SynFinance Fraud Detection ML Tutorial\n",
    "\n",
    "This notebook demonstrates the complete ML workflow for fraud detection using SynFinance:\n",
    "\n",
    "1. Generate synthetic transaction data with fraud patterns\n",
    "2. Engineer ML features from transaction history\n",
    "3. Create balanced training dataset\n",
    "4. Train fraud detection models (Random Forest & XGBoost)\n",
    "5. Evaluate model performance\n",
    "6. Analyze feature importance\n",
    "\n",
    "**Author**: SynFinance Development Team  \n",
    "**Version**: 0.5.0  \n",
    "**Date**: October 26, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c439df2",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import required libraries and configure SynFinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Add SynFinance to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# SynFinance imports\n",
    "from src.data_generator import DataGenerator\n",
    "from src.generators.fraud_patterns import FraudPatternGenerator\n",
    "from src.generators.ml_features import MLFeatureEngineer\n",
    "from src.generators.ml_dataset_generator import MLDatasetGenerator\n",
    "\n",
    "# ML library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# XGBoost (install if needed: pip install xgboost)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports complete\")\n",
    "print(f\"✓ XGBoost available: {XGBOOST_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fba2ff",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Transaction Data\n",
    "\n",
    "Create 1000 transactions with 10% fraud rate using various fraud patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb34443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data generator\n",
    "generator = DataGenerator(\n",
    "    num_customers=100,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    num_days=30\n",
    ")\n",
    "\n",
    "# Generate customer profiles\n",
    "print(\"Generating customer profiles...\")\n",
    "customers = generator.generate_customers()\n",
    "print(f\"✓ Generated {len(customers)} customers\")\n",
    "\n",
    "# Generate transactions\n",
    "print(\"\\nGenerating transactions...\")\n",
    "transactions = generator.generate_transactions(num_transactions=1000)\n",
    "print(f\"✓ Generated {len(transactions)} transactions\")\n",
    "\n",
    "# Inject fraud patterns\n",
    "print(\"\\nInjecting fraud patterns...\")\n",
    "fraud_gen = FraudPatternGenerator(seed=42)\n",
    "transactions = fraud_gen.inject_fraud_patterns(\n",
    "    transactions,\n",
    "    customers,\n",
    "    fraud_rate=0.10,\n",
    "    patterns=['card_cloning', 'velocity_abuse', 'geographic_impossible']\n",
    ")\n",
    "\n",
    "# Calculate fraud statistics\n",
    "fraud_count = sum(1 for t in transactions if t.get('is_fraud', 0) == 1)\n",
    "fraud_rate = fraud_count / len(transactions)\n",
    "\n",
    "print(f\"✓ Fraud injection complete\")\n",
    "print(f\"  - Total transactions: {len(transactions)}\")\n",
    "print(f\"  - Fraud transactions: {fraud_count}\")\n",
    "print(f\"  - Fraud rate: {fraud_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ac12f",
   "metadata": {},
   "source": [
    "## Step 2: Engineer ML Features\n",
    "\n",
    "Extract 32 features from transaction data across 6 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d716fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = MLFeatureEngineer()\n",
    "\n",
    "# Build transaction history lookup\n",
    "print(\"Building transaction history...\")\n",
    "history_lookup = {}\n",
    "for txn in transactions:\n",
    "    customer_id = txn['customer_id']\n",
    "    if customer_id not in history_lookup:\n",
    "        history_lookup[customer_id] = []\n",
    "    history_lookup[customer_id].append(txn)\n",
    "\n",
    "# Sort history by timestamp\n",
    "for customer_id in history_lookup:\n",
    "    history_lookup[customer_id].sort(key=lambda x: x['timestamp'])\n",
    "\n",
    "print(f\"✓ Built history for {len(history_lookup)} customers\")\n",
    "\n",
    "# Engineer features for each transaction\n",
    "print(\"\\nEngineering features...\")\n",
    "features_list = []\n",
    "\n",
    "for i, txn in enumerate(transactions):\n",
    "    # Get customer info\n",
    "    customer = next(c for c in customers if c['customer_id'] == txn['customer_id'])\n",
    "    \n",
    "    # Get transaction history (all transactions before this one)\n",
    "    customer_history = history_lookup[txn['customer_id']]\n",
    "    txn_index = customer_history.index(txn)\n",
    "    history = customer_history[:txn_index]\n",
    "    \n",
    "    # Engineer features\n",
    "    ml_features = feature_engineer.engineer_features(\n",
    "        transaction=txn,\n",
    "        customer=customer,\n",
    "        transaction_history=history\n",
    "    )\n",
    "    \n",
    "    features_list.append(ml_features.to_dict())\n",
    "    \n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(transactions)} transactions\")\n",
    "\n",
    "print(f\"\\n✓ Engineered features for {len(features_list)} transactions\")\n",
    "print(f\"  - Features per transaction: {len(features_list[0]) - 3}\")  # Exclude id, is_fraud, fraud_type\n",
    "\n",
    "# Display feature metadata\n",
    "metadata = feature_engineer.get_feature_metadata()\n",
    "print(f\"\\nFeature Categories:\")\n",
    "for category, features in metadata['features'].items():\n",
    "    print(f\"  - {category}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903519e5",
   "metadata": {},
   "source": [
    "## Step 3: Create ML-Ready Dataset\n",
    "\n",
    "Balance dataset, split into train/val/test, normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset generator\n",
    "dataset_gen = MLDatasetGenerator(seed=42)\n",
    "\n",
    "# Create ML-ready dataset\n",
    "print(\"Creating ML-ready dataset...\\n\")\n",
    "split, metadata = dataset_gen.create_ml_ready_dataset(\n",
    "    features_list,\n",
    "    balance_strategy='undersample',\n",
    "    target_fraud_rate=0.5,\n",
    "    normalize=True,\n",
    "    encode_categorical=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Dataset creation complete\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "stats = split.get_stats()\n",
    "print(f\"  Train set: {stats['train_size']} samples (fraud rate: {stats['train_fraud_rate']:.1%})\")\n",
    "print(f\"  Validation set: {stats['validation_size']} samples (fraud rate: {stats['validation_fraud_rate']:.1%})\")\n",
    "print(f\"  Test set: {stats['test_size']} samples (fraud rate: {stats['test_fraud_rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5b6ad",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data for ML Models\n",
    "\n",
    "Convert to numpy arrays for scikit-learn and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7844128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(dataset):\n",
    "    \"\"\"Convert dataset to X, y numpy arrays.\"\"\"\n",
    "    # Define feature columns (exclude ID and labels)\n",
    "    exclude_cols = {'transaction_id', 'is_fraud', 'fraud_type', 'fraud_type_encoded'}\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Get feature columns\n",
    "    all_cols = set(dataset[0].keys())\n",
    "    feature_cols = sorted(all_cols - exclude_cols)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = np.array([[sample.get(col, 0) for col in feature_cols] for sample in dataset])\n",
    "    y = np.array([sample.get('is_fraud', 0) for sample in dataset])\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing data for ML models...\")\n",
    "X_train, y_train, feature_names = prepare_ml_data(split.train)\n",
    "X_val, y_val, _ = prepare_ml_data(split.validation)\n",
    "X_test, y_test, _ = prepare_ml_data(split.test)\n",
    "\n",
    "print(f\"✓ Data preparation complete\")\n",
    "print(f\"  - Training features shape: {X_train.shape}\")\n",
    "print(f\"  - Validation features shape: {X_val.shape}\")\n",
    "print(f\"  - Test features shape: {X_test.shape}\")\n",
    "print(f\"  - Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e528fa",
   "metadata": {},
   "source": [
    "## Step 5: Train Random Forest Classifier\n",
    "\n",
    "Baseline model using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest Classifier...\\n\")\n",
    "\n",
    "# Initialize and train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "rf_test_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✓ Random Forest training complete\\n\")\n",
    "\n",
    "# Evaluate on train set\n",
    "print(\"Training Set Performance:\")\n",
    "print(f\"  F1-Score: {f1_score(y_train, rf_train_pred):.3f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_train, rf_model.predict_proba(X_train)[:, 1]):.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"  F1-Score: {f1_score(y_val, rf_val_pred):.3f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_val, rf_model.predict_proba(X_val)[:, 1]):.3f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, rf_test_pred):.3f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, rf_test_proba):.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, rf_test_pred, target_names=['Normal', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7ba9e",
   "metadata": {},
   "source": [
    "## Step 6: Train XGBoost Classifier\n",
    "\n",
    "Advanced gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"Training XGBoost Classifier...\\n\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    xgb_train_pred = xgb_model.predict(X_train)\n",
    "    xgb_val_pred = xgb_model.predict(X_val)\n",
    "    xgb_test_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    xgb_test_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"✓ XGBoost training complete\\n\")\n",
    "    \n",
    "    # Evaluate on train set\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  F1-Score: {f1_score(y_train, xgb_train_pred):.3f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_score(y_train, xgb_model.predict_proba(X_train)[:, 1]):.3f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nValidation Set Performance:\")\n",
    "    print(f\"  F1-Score: {f1_score(y_val, xgb_val_pred):.3f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_score(y_val, xgb_model.predict_proba(X_val)[:, 1]):.3f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  F1-Score: {f1_score(y_test, xgb_test_pred):.3f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_score(y_test, xgb_test_proba):.3f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report (Test Set):\")\n",
    "    print(classification_report(y_test, xgb_test_pred, target_names=['Normal', 'Fraud']))\n",
    "else:\n",
    "    print(\"XGBoost not available. Skipping XGBoost training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4f3a4",
   "metadata": {},
   "source": [
    "## Step 7: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c901ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "fig, axes = plt.subplots(1, 2 if XGBOOST_AVAILABLE else 1, figsize=(12 if XGBOOST_AVAILABLE else 6, 5))\n",
    "\n",
    "if not XGBOOST_AVAILABLE:\n",
    "    axes = [axes]\n",
    "\n",
    "# Random Forest confusion matrix\n",
    "cm_rf = confusion_matrix(y_test, rf_test_pred)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Random Forest Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_xticklabels(['Normal', 'Fraud'])\n",
    "axes[0].set_yticklabels(['Normal', 'Fraud'])\n",
    "\n",
    "# XGBoost confusion matrix\n",
    "if XGBOOST_AVAILABLE:\n",
    "    cm_xgb = confusion_matrix(y_test, xgb_test_pred)\n",
    "    sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "    axes[1].set_title('XGBoost Confusion Matrix')\n",
    "    axes[1].set_ylabel('True Label')\n",
    "    axes[1].set_xlabel('Predicted Label')\n",
    "    axes[1].set_xticklabels(['Normal', 'Fraud'])\n",
    "    axes[1].set_yticklabels(['Normal', 'Fraud'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✓ Saved confusion matrices to output/confusion_matrices.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661ad03",
   "metadata": {},
   "source": [
    "## Step 8: ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_test_proba)\n",
    "roc_auc_rf = roc_auc_score(y_test, rf_test_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.3f})', linewidth=2)\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_test_proba)\n",
    "    roc_auc_xgb = roc_auc_score(y_test, xgb_test_proba)\n",
    "    plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_xgb:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Fraud Detection Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('output/roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✓ Saved ROC curves to output/roc_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0badba67",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "rf_importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✓ Saved feature importance plot to output/feature_importance.png\")\n",
    "plt.show()\n",
    "\n",
    "# Display top 10\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for idx, row in feature_importance_df.head(10).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc709a1",
   "metadata": {},
   "source": [
    "## Step 10: Export Dataset to Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output/ml_exports', exist_ok=True)\n",
    "\n",
    "# 1. Export to CSV\n",
    "dataset_gen.export_to_csv(split.train, 'output/ml_exports/train.csv')\n",
    "dataset_gen.export_to_csv(split.test, 'output/ml_exports/test.csv')\n",
    "print(\"✓ Exported to CSV format\")\n",
    "\n",
    "# 2. Export to JSON\n",
    "dataset_gen.export_to_json(split.train, 'output/ml_exports/train.json')\n",
    "dataset_gen.export_to_json(split.test, 'output/ml_exports/test.json')\n",
    "print(\"✓ Exported to JSON format\")\n",
    "\n",
    "# 3. Export metadata\n",
    "dataset_gen.export_metadata('output/ml_exports/dataset_metadata.json', metadata)\n",
    "print(\"✓ Exported metadata\")\n",
    "\n",
    "# 4. Export to NumPy arrays\n",
    "np.save('output/ml_exports/X_train.npy', X_train)\n",
    "np.save('output/ml_exports/y_train.npy', y_train)\n",
    "np.save('output/ml_exports/X_test.npy', X_test)\n",
    "np.save('output/ml_exports/y_test.npy', y_test)\n",
    "print(\"✓ Exported to NumPy format\")\n",
    "\n",
    "# 5. Save feature names\n",
    "with open('output/ml_exports/feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "print(\"✓ Exported feature names\")\n",
    "\n",
    "print(\"\\n✓ All exports complete in output/ml_exports/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee8a8a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete ML workflow for fraud detection:\n",
    "\n",
    "**Key Results**:\n",
    "- Generated 1000 transactions with realistic fraud patterns\n",
    "- Engineered 32 features across 6 categories\n",
    "- Trained Random Forest and XGBoost models\n",
    "- Achieved strong fraud detection performance\n",
    "- Identified most important features for fraud prediction\n",
    "\n",
    "**Next Steps**:\n",
    "- Experiment with different fraud patterns\n",
    "- Tune model hyperparameters\n",
    "- Try ensemble methods\n",
    "- Deploy model for real-time fraud detection\n",
    "\n",
    "For more information, see the SynFinance documentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
